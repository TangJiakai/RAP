{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2023\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_df = pd.read_csv(\"raw-ml-1M/ml-1m.inter\", sep='\\t')\n",
    "item_df = pd.read_csv(\"raw-ml-1M/ml-1m.item\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_df = inter_df.rename(columns={\"user_id:token\":\"user_id\", \"item_id:token\":\"item_id\", \"rating:float\":\"rating\", \"timestamp:float\":\"timestamp\"})\n",
    "item_df = item_df.rename(columns={\"item_id:token\":\"item_id\", \"movie_title:token_seq\":\"movie_title\", \"release_year\":\"token\", \"class:token_seq\":\"class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706, 1000209)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter_df.user_id.nunique(), inter_df.item_id.nunique(), len(inter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9553163743776871"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - len(inter_df) / (inter_df.user_id.nunique() * inter_df.item_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_df = inter_df.drop_duplicates([\"user_id\", \"item_id\"], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inter_df = inter_df[inter_df[\"rating\"] > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_inter_num = 15\n",
    "\n",
    "while True:\n",
    "    clean_inter_df = clean_inter_df.groupby('user_id').filter(lambda x:len(x)>=threshold_inter_num)\n",
    "    clean_inter_df = clean_inter_df.groupby('item_id').filter(lambda x:len(x)>=threshold_inter_num)\n",
    "    if clean_inter_df.groupby('user_id').size().min() >= threshold_inter_num and clean_inter_df.groupby('item_id').size().min() >= threshold_inter_num:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5651 2555 564827\n"
     ]
    }
   ],
   "source": [
    "sizes = clean_inter_df.groupby(\"user_id\").size()\n",
    "print(clean_inter_df.user_id.nunique(), clean_inter_df.item_id.nunique(), len(clean_inter_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = item_df[np.isin(item_df.item_id, clean_inter_df['item_id'].values)]\n",
    "\n",
    "user_id_codes, user_id_uniques = pd.factorize(clean_inter_df['user_id'])\n",
    "clean_inter_df['user_id'] = user_id_codes\n",
    "item_id_codes, item_id_uniques = pd.factorize(clean_inter_df['item_id'])\n",
    "item_id_remap = dict(zip(item_id_uniques, np.arange(len(item_id_uniques))))\n",
    "clean_inter_df['item_id'] = item_id_codes\n",
    "item_df['item_id'] = item_df['item_id'].map(item_id_remap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_inter_df = clean_inter_df.sort_values(by=[\"user_id\", \"timestamp\"], axis=0)\n",
    "clean_inter_df = clean_inter_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_group(group, split_ratio=[0.8,0.1,0.1]):\n",
    "    num = len(group)\n",
    "    test_num = math.ceil(num * split_ratio[2])\n",
    "    valid_num = math.ceil(num * split_ratio[1])\n",
    "    train_num = num - test_num - valid_num\n",
    "    nums = [train_num, valid_num, test_num]\n",
    "    offsets = [0] + list(np.cumsum(nums))\n",
    "    splits = [group.iloc[offsets[i]:offsets[i+1]] for i in range(len(nums))]\n",
    "    return splits\n",
    "\n",
    "splits = clean_inter_df.groupby(by=\"user_id\").apply(split_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_inter, valid_pos_inter, test_pos_inter = [pd.concat([s[i] for s in splits]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58961, 58961, 446905)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_pos_inter), len(test_pos_inter), len(train_pos_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_noise_inters(df):\n",
    "    sample_index = np.random.choice(df.index.values, size=int(noise_ratio * len(df)))\n",
    "    sample_user_ids = df.loc[sample_index, 'user_id'].values\n",
    "    sample_item_ids = df.loc[sample_index, 'item_id'].values\n",
    "    item_num = df.item_id.nunique()\n",
    "    user2posItem_dict = df.groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "\n",
    "    check_index = np.arange(len(sample_index))\n",
    "    sample_item_ids = np.zeros_like(check_index)\n",
    "\n",
    "    while len(check_index) > 0:\n",
    "        sample_item_ids[check_index] = np.random.randint(0, item_num, size=len(check_index))\n",
    "        check_index = [\n",
    "            i\n",
    "            for i, u, sample_item_id in zip(check_index, sample_user_ids[check_index], sample_item_ids[check_index])\n",
    "            if sample_item_id in user2posItem_dict[u]\n",
    "        ]\n",
    "    \n",
    "    df.loc[sample_index, 'item_id'] = sample_item_ids\n",
    "    df.loc[sample_index, 'noise_flag'] = 1\n",
    "    return df\n",
    "\n",
    "train_pos_inter['noise_flag'] = 0\n",
    "train_pos_inter = inject_noise_inters(train_pos_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter valid and test data, where item_id is not appeared in train dataset\n",
    "train_used_item_set = train_pos_inter.item_id.unique()\n",
    "valid_pos_inter = valid_pos_inter[np.isin(valid_pos_inter.item_id, train_used_item_set, assume_unique=True)]\n",
    "test_pos_inter = test_pos_inter[np.isin(test_pos_inter.item_id, train_used_item_set, assume_unique=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag = (sum(np.isin(item_df.item_id, clean_inter_df['item_id'].unique())==False) > 0)\n",
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe some items will be removed due to noise injecting operation, we should remap item id again\n",
    "if flag:\n",
    "    item_df = item_df[np.isin(item_df.item_id, clean_inter_df['item_id'].unique())]\n",
    "\n",
    "    item_id_codes, item_id_uniques = pd.factorize(clean_inter_df['item_id'])\n",
    "    itemId_map_dict = dict(zip(item_id_uniques, np.arange(len(item_id_uniques))))\n",
    "    clean_inter_df['item_id'] = item_id_codes\n",
    "    valid_pos_inter['item_id'] = valid_pos_inter.item_id.map(itemId_map_dict)\n",
    "    test_pos_inter['item_id'] = test_pos_inter.item_id.map(itemId_map_dict)\n",
    "    item_df['item_id'] = item_df['item_id'].map(itemId_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58961, 58961, 446905)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_pos_inter), len(test_pos_inter), len(train_pos_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------user----------\n",
      "11 1074\n",
      "2 135\n",
      "2 135\n",
      "----------item----------\n",
      "29 1797\n",
      "1 311\n",
      "1 284\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"-\"*10 + \"user\" + \"-\"*10)\n",
    "sizes = train_pos_inter.groupby(\"user_id\").size()\n",
    "print(sizes.min(), sizes.max())\n",
    "sizes = valid_pos_inter.groupby(\"user_id\").size()\n",
    "print(sizes.min(), sizes.max())\n",
    "sizes = test_pos_inter.groupby(\"user_id\").size()\n",
    "print(sizes.min(), sizes.max())\n",
    "\n",
    "print(\"-\"*10 + \"item\" + \"-\"*10)\n",
    "sizes = train_pos_inter.groupby(\"item_id\").size()\n",
    "print(sizes.min(), sizes.max())\n",
    "sizes = valid_pos_inter.groupby(\"item_id\").size()\n",
    "print(sizes.min(), sizes.max())\n",
    "sizes = test_pos_inter.groupby(\"item_id\").size()\n",
    "print(sizes.min(), sizes.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_inter.to_csv(f\"pro-ml-1M/ml-1M-train.{noise_ratio}\", sep='\\t', index=False)\n",
    "valid_pos_inter.to_csv(f\"pro-ml-1M/ml-1M-valid.{noise_ratio}\", sep='\\t', index=False)\n",
    "test_pos_inter.to_csv(f\"pro-ml-1M/ml-1M-test.{noise_ratio}\", sep='\\t', index=False)\n",
    "item_df.to_csv(f\"pro-ml-1M/ml-1M-item.{noise_ratio}\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('recbole': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b9f566fea22d9af766201cc1c455ad10a071273f07c4072f15114646e19bc2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
